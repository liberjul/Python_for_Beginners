{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python for Beginners Exercise 6: Machine Learning 1 - Sklearn\n",
    "\n",
    "Made by: Julian Liber\n",
    "\n",
    "Date Created: 03/17/2020\n",
    "\n",
    "## Hello Everyone!\n",
    "\n",
    "\n",
    "#### This activity should teach you:\n",
    "- What machine learning is\n",
    "- What supervised machine learning is and requires\n",
    "- What is the difference between classification and regression.\n",
    "- How to train a basic ML model\n",
    "- How to interpret ML results\n",
    "\n",
    "Machine learning (ML) is a often-discussed topic in computer science. While it is simply another statistical method for understanding your data, ML can be useful for tasks such as classification and regression.\n",
    "\n",
    "ML can be either supervised or unsupervised. Supervised methods use variables (called _features_) to predict one or more dependent variables, based on known observations of independent and dependent variables. Unsupervised methods find relationships between observations based only features.\n",
    "\n",
    "Here are some of each type\n",
    "\n",
    "#### Supervised methods:\n",
    "- Classification\n",
    "- Regression\n",
    "\n",
    "#### Unsupervised methods:\n",
    "- Clustering\n",
    "- Ordination\n",
    "\n",
    "We want to be careful to not do this however:\n",
    "\n",
    "<img src=\"https://imgs.xkcd.com/comics/machine_learning.png\" width=50% alt=\"Hulahoop\"><p style=\"text-align: right;\">From: https://imgs.xkcd.com/comics/machine_learning.png</p>\n",
    "\n",
    "We want ML to make **useful** predictions, so therefore we try to avoid problems of any statistical model, specifically overfitting and underfitting.\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_underfitting_overfitting_001.png\" width=100% alt=\"Hulahoop\"><p style=\"text-align: right;\">From: https://scikit-learn.org/stable/_images/sphx_glr_plot_underfitting_overfitting_001.png</p>\n",
    "\n",
    "\n",
    "To help do this, we will often split the data into _training_ and _testing_ data, to ensure that we improve the ability of the model to predict in the real world.\n",
    "\n",
    "#### First ML exercise\n",
    "\n",
    "Our first exercise will be a classification problem. We are using the [seeds](https://archive.ics.uci.edu/ml/datasets/seeds) dataset, wich compares seed traits of 3 different wheat varieties.\n",
    "\n",
    "A classification problem attempts to predict _labels_ given _features_. In this case the wheat variety (Kama, Rosa, or Canadian) are the labels, which are predicted using these seed traits (from the UCI website):\n",
    "\n",
    "1. area A,\n",
    "2. perimeter P,\n",
    "3. compactness C = 4*pi*A/P^2,\n",
    "4. length of kernel,\n",
    "5. width of kernel,\n",
    "6. asymmetry coefficient\n",
    "7. length of kernel groove.\n",
    "\n",
    "For our first steps, we need to do these steps:\n",
    "1. Read in the data\n",
    "2. Split in features and labels\n",
    "3. Split into training and testing data\n",
    "4. Train the model\n",
    "5. Test the model\n",
    "\n",
    "We are going to do this first using a Support Vector Machine (SVM), which attempts to linearly separate groups. More can be learned [here](https://scikit-learn.org/stable/modules/svm.html#classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read in the data\n",
    "full_data = np.empty((8,))\n",
    "with open(\"seeds_dataset.txt\", \"r\") as datafile:\n",
    "    line = datafile.readline()\n",
    "    while line != \"\":\n",
    "        line_dat = np.array(line.split(\"\\t\"))\n",
    "        line_dat = line_dat[line_dat != \"\"].astype(\"float\")\n",
    "        full_data = np.vstack((full_data, line_dat))\n",
    "        line = datafile.readline()\n",
    "# 2. Split into features and labels\n",
    "x = full_data[:,0:7]\n",
    "y = full_data[:,7]\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Split into training and testing data \n",
    "train_feat, test_feat, train_lab, test_lab = train_test_split(x, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_feat.shape, test_feat.shape)\n",
    "print(train_lab.shape, test_lab.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=train_feat[:,0], y=train_feat[:,5], c=train_lab)\n",
    "plt.xlabel(\"Seed Area\")\n",
    "plt.ylabel(\"Asymmetry Coefficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data here show rather good differentiation between the varieties, so it should be possible to classify them with a machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Train the model\n",
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "clf.fit(train_feat, train_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Test the model\n",
    "pred_lab = clf.predict(test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the results\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "plt.scatter(x = test_feat[:,0], y = test_feat[:,5], c = test_lab)\n",
    "plt.xlabel(\"Seed Area\")\n",
    "plt.ylabel(\"Asymmetry Coefficient\")\n",
    "plt.title(\"True Labels\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(x = test_feat[:,0], y = test_feat[:,5], c = pred_lab)\n",
    "plt.xlabel(\"Seed Area\")\n",
    "plt.ylabel(\"Asymmetry Coefficient\")\n",
    "plt.title(\"Predicted Labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we conclude from this visual analysis?\n",
    "- Looks good for the points well within the group\n",
    "- Performs poorly for points outside the groups\n",
    "- Entirely linearly split\n",
    "\n",
    "What if we want a more quantitative analysis?\n",
    "- Classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_lab, pred_lab))\n",
    "print(metrics.confusion_matrix(test_lab, pred_lab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good, right?\n",
    "\n",
    "It all depends on your goal here, but precision and recall are pretty balanced.\n",
    "\n",
    "Additionally, the F1 score is the harmonic mean of precision and recall, so can be a good way to show the balance between both. This [wiki page](https://en.wikipedia.org/wiki/Precision_and_recall) has a good explanation of what this means.\n",
    "\n",
    "${\\displaystyle \\mathrm {F} _{1}=2\\cdot {\\frac {\\mathrm {PPV} \\cdot \\mathrm {TPR} }{\\mathrm {PPV} +\\mathrm {TPR} }}={\\frac {2\\mathrm {TP} }{2\\mathrm {TP} +\\mathrm {FP} +\\mathrm {FN} }}}$\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/525px-Precisionrecall.svg.png\" width=50% alt=\"Hulahoop\"><p style=\"text-align: right;\">From: https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/525px-Precisionrecall.svg.png</p>\n",
    "\n",
    "Let's try another model: the RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf2 = RandomForestClassifier()\n",
    "clf2.fit(train_feat, train_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lab2 = clf2.predict(test_feat)\n",
    "print(metrics.classification_report(test_lab, pred_lab2))\n",
    "print(metrics.confusion_matrix(test_lab, pred_lab2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the performance of the classification is similar, the RandomForest algorithm allows for an additional analysis step: _feature importances_.\n",
    "\n",
    "Feature importances tell how influencial each feature is in the classification decision, which can be interesting if you want to know what matters to the model's decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(y = np.arange(len(clf2.feature_importances_)),\n",
    "        width=clf2.feature_importances_,\n",
    "        tick_label = [\"area\", \"perimeter\",\"compactness\" ,\"kernel length\", \"kernel width\", \"asymmetry coef\", \"groove length\"],\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do This:\n",
    "Find another machine learning algorithm to try. [This page](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning) has many to attempt to implement.\n",
    "\n",
    "Fit the model with the training data, and use the testing data to assess its success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks for doing Exercise 6!\n",
    "\n",
    "#### More will follow soon!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
